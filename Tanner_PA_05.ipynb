{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084a5836-46c8-47ac-a1e0-e8348a525150",
   "metadata": {},
   "source": [
    "# ProgrammingAssignment05_clusterAnalysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf8149-9aa3-4627-8f2c-49e531f9c5e9",
   "metadata": {},
   "source": [
    "## 1. k-means using scikit-learn\n",
    "The healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply k-means clustering to the cities' number of hours of sunshine and happiness levels.\n",
    "\n",
    "- Import the needed packages for clustering.\n",
    "- Initialize and fit a k-means clustering model using sklearn's Kmeans() function.\n",
    "- Use the user-defined number of clusters, init='random', n_init=10, random_state=123, and algorithm='elkan'.\n",
    "- Find the cluster centroids and inertia.\n",
    "\n",
    "Ex: If the input is: 4\n",
    "\n",
    "the output should be:\n",
    "\n",
    "- Centroids: [[ 0.8294  0.2562]\n",
    " [ 1.3106 -1.887 ]\n",
    " [-0.9471  0.8281]\n",
    " [-0.6372 -0.7943]]\n",
    "- Inertia: 16.4991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54189b-81b1-4a58-9dd8-42ca5ba05310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "healthy = pd.read_csv('healthy_lifestyle.csv')\n",
    "\n",
    "# Input the number of clusters\n",
    "number = int(input())\n",
    "\n",
    "# Define input features\n",
    "X = healthy[['sunshine_hours', 'happiness_levels']]\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])\n",
    "X = X.dropna()\n",
    "\n",
    "# Initialize a k-means clustering algorithm with a user-defined number of clusters, init='random', n_init=10, \n",
    "# random_state=123, and algorithm='elkan'\n",
    "kmeans = KMeans(n_clusters=number, init='random', n_init=10,\n",
    "                random_state=123, algorithm='elkan')\n",
    "\n",
    "# Fit the algorithm to the input features\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Find and print the cluster centroids\n",
    "centroid = kmeans.cluster_centers_\n",
    "print(\"Centroids:\", np.round(centroid,4))\n",
    "\n",
    "# Find and print the cluster inertia\n",
    "inertia = kmeans.inertia_\n",
    "print(\"Inertia:\", np.round(inertia,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18700cd6-8069-458a-8822-61f755fd893e",
   "metadata": {},
   "source": [
    "## 2. Hierarchical clustering using scikit-learn\n",
    "The healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply agglomerative clustering to the cities' number of hours of sunshine and happiness levels using both sklearn and SciPy.\n",
    "\n",
    "- Import the needed packages for agglomerative clustering from sklearn and SciPy.\n",
    "- Initialize and fit an agglomerative clustering model using sklearn's AgglomerativeClustering() function. Use the user-defined number of clusters and ward linkage.\n",
    "- Add cluster labels to the input feature dataframe.\n",
    "- Calculate the distances between all instances using SciPy's pdist() function.\n",
    "- Convert the distance matrix to a square matrix using SciPy's squareform() function.\n",
    "- Define a clustering model with ward linkage using SciPy's linkage() function.\n",
    "\n",
    "Ex: If the input is: 4\n",
    "\n",
    "the output should be:\n",
    "|       | sunshine_hours | happiness_levels | labels |\n",
    "|-------|----------------|------------------|--------|\n",
    "| 0     | -0.691660      | 1.025642         | 3      |\n",
    "| 1     | 0.695725       | 0.801124         | 0      |\n",
    "| 2     | -0.645295      | 0.872562         | 3      |\n",
    "| 3     | -0.757641      | 0.933794         | 3      |\n",
    "| 4     | -1.098246      | 1.229750         | 3      |\n",
    "\n",
    "\n",
    "First five rows of the linkage matrix from SciPy:\n",
    "    \n",
    " - [[39. 40.  0.  2.]\n",
    " [28. 43.  0.  3.]\n",
    " [ 7. 18.  0.  2.]\n",
    " [ 8. 42.  0.  2.]\n",
    " [ 0.  3.  0.  2.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d1471-d33e-43b5-839c-ba86c9509ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Import needed sklearn packages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Import needed scipy packages\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Silence warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "healthy = pd.read_csv('healthy_lifestyle.csv')\n",
    "\n",
    "# Input the number of clusters\n",
    "number = int(input())\n",
    "\n",
    "# Define input features\n",
    "X = healthy[['sunshine_hours', 'happiness_levels']]\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])\n",
    "X = X.dropna()\n",
    "\n",
    "# Initialize and fit an agglomerative clustering model using ward linkage in scikit-learn, with a user-defined\n",
    "# number of clusters\n",
    "agg = AgglomerativeClustering(n_clusters=number, linkage='ward')\n",
    "agg.fit(X_df)\n",
    "\n",
    "# Add cluster labels to input feature dataframe\n",
    "X['labels']=# agg.labels_\n",
    "print(X.head())\n",
    "\n",
    "# Perform agglomerative clustering using SciPy\n",
    "\n",
    "# Calculate the distances between all instances\n",
    "dist_matrix = pdist(X_df[['sunshine_hours', 'happiness_levels']])\n",
    "\n",
    "# Convert the distance matrix to a square matrix\n",
    "square_dist = squareform(dist_matrix)\n",
    "\n",
    "# Define a clustering model with ward linkage\n",
    "clustersHealthyScipy = linkage(dist_matrix, method='ward')\n",
    "\n",
    "print('First five rows of the linkage matrix from SciPy:\\n', np.round(clustersHealthyScipy[:5, :], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db51dd-7ad3-4f58-b0ff-ae71a6c306e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d92b04d3-05bd-470c-a199-327a6b55679c",
   "metadata": {},
   "source": [
    "## 3. DBSCAN using scikit-learn\n",
    "- Increase the **number of points sampled to 500**.\n",
    "- Apply the DBSCAN model with **epsilon=1** and **min_samples=8** to identify the number of core-points and outliers (or noise). \n",
    "- EX: if the epsilon=1 and min_samples = 10 and number of points sampled to 100.\n",
    "  - the number of core-points = 85\n",
    "  - the number of outliers    = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeae7b7-d4e7-483a-81bc-1d4bd4d6716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load the full grocery customer dataset and take a random sample of 500 instances\n",
    "data = pd.read_csv('customer_personality.csv').sample( # your code here , random_state=123)\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "X = data[['Fruits', 'Meats']]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Apply DBSCAN with epsilon=1 and min_samples = 8   \n",
    "dbscan = DBSCAN(eps=1, min_samples=8)\n",
    "dbscan = dbscan.fit(X)\n",
    "\n",
    "# Print the cluster labels and core point indices\n",
    "print('Labels:', dbscan.labels_ )\n",
    "print('Core points:', dbscan.core_sample_indices_) \n",
    "print('Number of core points:', len(dbscan.core_sample_indices_) )\n",
    "\n",
    "# Add the cluster labels to the dataset as strings\n",
    "data['clusters'] = dbscan.labels_.astype(str)\n",
    "\n",
    "# Sort by cluster label (for plotting purposes)\n",
    "data.sort_values(by='clusters', inplace=True)\n",
    "\n",
    "# Plot clusters on the original data\n",
    "p = sns.scatterplot(data=data, x='Fruits',\n",
    "                    y='Meats', hue='clusters',\n",
    "                    style='clusters')\n",
    "p.set_xlabel('Fruits', fontsize=16)\n",
    "p.set_ylabel('Meats', fontsize=16)\n",
    "p.legend(title='DBSCAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea545b6-c934-42ca-9f5a-be7bf69d22d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
